{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a character-level GPT on some text data\n",
        "\n",
        "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Overview:** This demo trains a character-level GPT (Generative Pre-trained Transformer) on Shakespeare text. Unlike word-level models that operate on tokens, a char-level model predicts the next *character* at each step. This makes the vocabulary small (~65 chars for English) but sequences much longer. The minGPT architecture is the same as GPT-2: stacked Transformer decoder blocks with causal (masked) self-attention.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Logging setup:** Configures Python's logging module to print timestamps, log levels, and messages. Useful for tracking training progress and debugging. The format shows when each log line was produced.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make deterministic\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Reproducibility:** `set_seed(42)` fixes the random seed for NumPy, PyTorch, and Python's random module. This ensures that training and sampling produce the same results across runs, which is essential for reproducible experiments.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Imports:** Standard libraries for the demo: NumPy for arrays, PyTorch for tensors and neural networks, and `F` (functional) for operations like softmax used in attention.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dataset.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data))) # get all unique characters\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # {'a': 0, 'b': 1, 'c': 2, ...}\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) } # {0: 'a', 1: 'b', 2: 'c', ...}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        return len(data) the training sample size\n",
        "\n",
        "        each sample is a chunk of (block_size + 1) characters\n",
        "        so, the start point of the samples could be 0, 1, 2, ..., len(data) - (block_size + 1)\n",
        "        Totally len(data) - (block_size + 1) + 1  = len(data) - block_size samples\n",
        "        \"\"\"\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1] # idx to idx + block_size + 1\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk] # [0, 1, 2, ..., block_size]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long) # input sequence [0, 1, 2, ..., block_size-1]\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # target sequence (predict next character) [1, 2, 3, ..., block_size]\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**CharDataset:** A PyTorch `Dataset` that:\n",
        "- Builds a character vocabulary (`stoi` = char→index, `itos` = index→char)\n",
        "- Slices the text into overlapping chunks of length `block_size + 1`\n",
        "- Returns `(x, y)` where `x` = first `block_size` chars, `y` = next `block_size` chars (shifted by 1). This implements *teacher forcing*: given chars 1..T, predict char 2..T+1\n",
        "- Enables efficient batched training: one forward pass trains the model on many positions at once (B×T predictions per batch). At inference, we generate one token at a time autoregressively.\n",
        "\n",
        "---\n",
        "\n",
        "**A concrete “drawn” alignment (what one sample trains):**\n",
        "\n",
        "Assume:\n",
        "- `block_size = T = 4`\n",
        "- A single text chunk is `\"hello\"` (length \\(T+1=5\\))\n",
        "- So `x = \"hell\"` and `y = \"ello\"`\n",
        "\n",
        "```\n",
        "chunk (T+1):   h    e    l    l    o\n",
        "index t:      0    1    2    3    4\n",
        "              |---- x ----|           (x has length T)\n",
        "                   |---- y ----|      (y has length T)\n",
        "\n",
        "x (input):     h    e    l    l\n",
        "y (target):    e    l    l    o\n",
        "pos in x/y:    0    1    2    3\n",
        "```\n",
        "\n",
        "At each position `pos` (0..T-1), the model does a next-character classification:\n",
        "\n",
        "```\n",
        "pos=0: input prefix = \"h\"        -> predict y[0] = \"e\"\n",
        "pos=1: input prefix = \"he\"       -> predict y[1] = \"l\"\n",
        "pos=2: input prefix = \"hel\"      -> predict y[2] = \"l\"\n",
        "pos=3: input prefix = \"hell\"     -> predict y[3] = \"o\"\n",
        "```\n",
        "\n",
        "Even though we feed the whole `x=\"hell\"` in one shot, *causal masking* ensures that the computation at `pos=k` can only see `x[:k+1]` (no peeking at future characters).\n",
        "\n",
        "---\n",
        "\n",
        "**What changes with batching:**\n",
        "\n",
        "Let batch size be `B = 2`, `T = 4`. Suppose we sampled two chunks:\n",
        "- Sample 0 chunk: `\"hello\"` → `x0=\"hell\"`, `y0=\"ello\"`\n",
        "- Sample 1 chunk: `\"orld!\"` → `x1=\"orld\"`, `y1=\"rld!\"`\n",
        "\n",
        "Then the batch tensors look like:\n",
        "\n",
        "```\n",
        "X (B,T) =\n",
        "  [ h  e  l  l ]   # sample 0\n",
        "  [ o  r  l  d ]   # sample 1\n",
        "\n",
        "Y (B,T) =\n",
        "  [ e  l  l  o ]   # sample 0\n",
        "  [ r  l  d  ! ]   # sample 1\n",
        "```\n",
        "\n",
        "Training computes `B*T` next-char predictions at once. You can think of it as this table of tasks:\n",
        "\n",
        "```\n",
        "                time position (pos)\n",
        "              0            1             2              3\n",
        "sample 0   \"h\" -> e    \"he\" -> l     \"hel\" -> l     \"hell\" -> o\n",
        "sample 1   \"o\" -> r    \"or\" -> l     \"orl\" -> d     \"orld\" -> !\n",
        "```\n",
        "\n",
        "So one forward/backward pass trains on \\(B\\times T\\) supervised targets simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "**Why test-time / generation is different:**\n",
        "\n",
        "At generation time, you only have a prompt and the future characters do not exist yet, so you must generate them step-by-step:\n",
        "\n",
        "```\n",
        "start prompt: \"he\"\n",
        "\n",
        "step 1:\n",
        "  input  = \"he\"\n",
        "  model  -> distribution for next char\n",
        "  pick   -> \"l\"\n",
        "  new sequence = \"hel\"\n",
        "\n",
        "step 2:\n",
        "  input  = \"hel\"\n",
        "  model  -> distribution for next char\n",
        "  pick   -> \"l\"\n",
        "  new sequence = \"hell\"\n",
        "\n",
        "step 3:\n",
        "  input  = \"hell\"\n",
        "  model  -> distribution for next char\n",
        "  pick   -> \"o\"\n",
        "  new sequence = \"hello\"\n",
        "```\n",
        "\n",
        "You cannot parallelize across these newly generated time steps because each step depends on the previous generated output (while training can parallelize across the *existing* positions of `x` using teacher forcing + causal masking).\n",
        "\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "block_size = 128 # spatial extent of the model for its context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**block_size (context length):** The maximum number of characters the model can \"see\" at once. With `block_size=128`, each input is 128 chars; the model uses causal masking so position `i` only attends to positions 1..i. Larger block_size = more context but more memory and compute.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1089k  100 1089k    0     0  2566k      0 --:--:-- --:--:-- --:--:-- 2562k\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt\n",
        "# -o = output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Data download:** Fetches the Tiny Shakespeare dataset (~1M characters) from Karpathy's char-rnn repo. This is a subset of Shakespeare's works used for quick language modeling experiments.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ],
      "source": [
        "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
        "from dataset import CharDataset\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1])\n",
            "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
            "        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
            "        44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
            "        54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
            "         1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,  1,\n",
            "        56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58, 53,\n",
            "         1, 42])\n"
          ]
        }
      ],
      "source": [
        "# my try\n",
        "x0,y0 = train_dataset.__getitem__(idx = 0)\n",
        "print(x0)\n",
        "print(y0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Loading the dataset:** Reads the raw text file and instantiates `CharDataset`. The output shows 1,115,394 characters and 65 unique characters (letters, punctuation, newlines, etc.). The dataset will produce `len(text) - block_size` training examples via overlapping windows.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "02/22/2026 17:06:44 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
          ]
        }
      ],
      "source": [
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=512)\n",
        "model = GPT(mconf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Model architecture:** Creates a GPT model with `GPTConfig`:\n",
        "- `vocab_size`, `block_size`: from the dataset\n",
        "- `n_layer=8`, `n_head=8`, `n_embd=512`: 8 Transformer blocks, 8 attention heads, 512-dimensional embeddings. This yields ~25M parameters. The model uses learned positional embeddings and causal self-attention in each block.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 0: train loss 4.32059. lr 5.999999e-04:   0%|          | 1/2179 [04:23<159:14:34, 263.21s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m tconf \u001b[38;5;241m=\u001b[39m TrainerConfig(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m,\n\u001b[1;32m      5\u001b[0m                       lr_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, warmup_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m, final_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m*\u001b[39mblock_size,\n\u001b[1;32m      6\u001b[0m                       num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_dataset, \u001b[38;5;28;01mNone\u001b[39;00m, tconf)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Repo/CS224n/a4/mingpt-demo/mingpt/trainer.py:122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# counter used for learning rate decay\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m run_epoch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Repo/CS224n/a4/mingpt-demo/mingpt/trainer.py:90\u001b[0m, in \u001b[0;36mTrainer.train.<locals>.run_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m     92\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cs224n-cpu/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cs224n-cpu/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Training:** The `Trainer` handles the training loop. Key config:\n",
        "- `max_epochs=2`: 2 full passes over the data\n",
        "- `batch_size=512`: 512 sequences per batch\n",
        "- `learning_rate=6e-4` with decay: common for Transformers\n",
        "- `warmup_tokens`, `final_tokens`: learning rate schedule (warmup then decay)\n",
        "- `num_workers=4`: data loading workers (set to 0 if multiprocessing fails locally)\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O God, O God! that e'er this tongue of mine,\n",
            "That laid the sentence of dread banishment\n",
            "On yon proud man, should take it off again\n",
            "With words of sooth! O that I were as great\n",
            "As is my grief, or lesser than my name!\n",
            "Or that I could forget what I have been,\n",
            "Or not remember what I must be now!\n",
            "Swell'st thou, proud heart? I'll give thee scope to beat,\n",
            "Since foes have scope to beat both thee and me.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Northumberland comes back from Bolingbroke.\n",
            "\n",
            "KING RICHARD II:\n",
            "What must the king do now? must he submit?\n",
            "The king shall do it: must he be deposed?\n",
            "The king shall be contented: must he lose\n",
            "The name of king? o' God's name, let it go:\n",
            "I'll give my jewels for a set of beads,\n",
            "My gorgeous palace for a hermitage,\n",
            "My gay apparel for an almsman's gown,\n",
            "My figured goblets for a dish of wood,\n",
            "My sceptre for a palmer's walking staff,\n",
            "My subjects for a pair of carved saints\n",
            "And my large kingdom for a little grave,\n",
            "A little little grave, an obscure grave;\n",
            "Or I'll be buried in the king's highway,\n",
            "Some way of common trade, where subjects' feet\n",
            "May hourly trample on their sovereign's head;\n",
            "For on my heart they tread now whilst I live;\n",
            "And buried once, why not upon my head?\n",
            "Aumerle, thou weep'st, my tender-hearted cousin!\n",
            "We'll make foul weather with despised tears;\n",
            "Our sighs and they shall lodge the summer corn,\n",
            "And make a dearth in this revolting land.\n",
            "Or shall we play the wantons with our woes,\n",
            "And make some pretty match with shedding tears?\n",
            "As thus, to drop them still upon one place,\n",
            "Till they have fretted us a pair of graves\n",
            "Within the earth; and, therein laid,--there lies\n",
            "Two kinsmen digg'd their graves with weeping eyes.\n",
            "Would not this ill do well? Well, well, I see\n",
            "I talk but idly, and you laugh at me.\n",
            "Most mighty prince, my Lord Northumberland,\n",
            "What says King Bolingbroke? will his majesty\n",
            "Give Richard leave to live till Richard die?\n",
            "You make a leg, and Bolingbroke says ay.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "My lord, in the base court he doth attend\n",
            "To speak with you; may it please you to come dow\n"
          ]
        }
      ],
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color: blue;\">\n",
        "\n",
        "**Sampling:** Autoregressive generation:\n",
        "1. Encode the prompt \"O God, O God!\" to token indices\n",
        "2. `sample()` repeatedly feeds the sequence to the model, gets the next-character distribution, samples from it (with `temperature=1.0`, `top_k=10` for diversity), appends to the sequence\n",
        "3. Decode indices back to characters. `temperature` controls randomness (higher = more random); `top_k` restricts sampling to the top-k most likely tokens.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# well that was fun"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs224n-cpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
