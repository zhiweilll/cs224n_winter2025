sents = [['hello', 'world'], ['hello', 'world', 'world'], ['hello', 'world', 'world', 'world']]

print(sents)

sents[1]

max_len = max(len(sents[i]) for i in range(len(sents)))

sents[1].append('<pad>')
print(sents[1])

sents[1].append('<pad>'*2)
print(sents[1])

sents[1] += ['<pad>'] * (max_len - len(sents[1]))
print(sents[1])

sent_pad = sents[1].extend(['<pad>'] * (max_len - len(sents[1])))
print(sents[1])
print(sent_pad)

pad_sents(sents, '0')


# Attention Part
# Encoder outputs: [h₁, h₂, h₃, ..., hₙ]   eahc hᵢ is 2*hidden_size

#            ↓ calaulate attention scores（and the similarity with each decoder hidden）

# Attention scores: [s₁, s₂, s₃, ..., sₙ]   each sᵢ is scale

#            ↓ Softmax

# Attention weights: [α₁, α₂, α₃, ..., αₙ]   sum = 1，indicate "attention"

#            ↓ weighted sum

# Context vector = α₁·h₁ + α₂·h₂ + ... + αₙ·hₙ   dim = 2*hidden_size



